# -*- coding: utf-8 -*-
"""HAR_Video_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B3Rgu-R-xrxYtJ_UAY-2rYL1C35Twh4D

## Load libraries
"""

from keras.models import Sequential
from keras.layers.core import Dense, Flatten, Dropout
from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D, MaxPooling2D, Conv2D
from keras.layers.recurrent import LSTM
from keras.layers import TimeDistributed, Activation, BatchNormalization, regularizers
import os
import cv2
import datetime
import numpy as np
from sklearn.model_selection import train_test_split
from keras.utils.vis_utils import plot_model
from sklearn.metrics import classification_report, accuracy_score
from keras.utils import np_utils
import matplotlib.pyplot as plt
from keras import optimizers
from keras.callbacks import CSVLogger


"""## Experiment scenario & parameter"""

scenario = 1 # 1 (3D-CNN) / 2 (C-RNN) / 3 (Pose-RNN)

image_scale = 1
frame_sequences = 20
actions = ['walking', 'handwaving', 'boxing']
random_state = 1
batch_size = 10
number_epoch = 20
test_size = 0.2
original_height = 120
original_width = 160
channel = 1

"""## Load dataset"""

# menampung seluruh dataset dan label
dataset_raw = []
labels_raw = []

for idx, action in enumerate(actions):
    path_dir = 'dataset/{}'.format(action)
    vids = os.listdir(path_dir)
    
    # iterates over all data automatically
    for vid in vids:
        if vid.endswith(".avi"):
            path_file = '{}/{}'.format(path_dir, vid)
            print(path_file)

            frames = []
            cap = cv2.VideoCapture(path_file)
            fps = cap.get(cv2.CAP_PROP_FPS)
            # width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
            # height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
            fps2 = int(1000 / fps)

            while True:
                # membaca video frame by frame
                ret, frame = cap.read()
                if ret:
                    # meresize frame
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    frames.append(frame)
                    if cv2.waitKey(fps2) & 0xFF == ord('q'):
                        break
                else:
                    break

            cv2.destroyAllWindows()

            dataset_raw.append(frames)
            labels_raw.append(idx)


"""## Preprocessing"""

"""### Generate image sequences (for 3D-CNN and C-RNN)"""
dataset = []
labels = []

if scenario == 1 or scenario == 2:
    for i, frames in enumerate(dataset_raw):
      tempframe = []
      action = labels_raw[i]
      for j in range(len(frames)):
          # memotong frame2 menjadi sejumlah frame_depth
          if len(tempframe) < frame_sequences:
              tempframe.append(frames[j])
          elif len(tempframe) >= frame_sequences:
              tempp = np.array(tempframe)
              dataset.append(tempp)
              labels.append(action)
              del tempframe[:]


# """### Extract pose features & generate pose sequences (for Pose-RNN)"""

elif scenario == 3:
    pass

"""## Model architecture"""

"""### 3D-CNN"""

# 3D Convolutional Neural Network

def c3d_model():
    """
    Build a 3D convolutional network, aka C3D.
        https://arxiv.org/pdf/1412.0767.pdf
    With thanks:
        https://gist.github.com/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2
    """
    input_shape = (frame_sequences, original_height, original_width, channel)

    model = Sequential()

    # 1st layer group
    model.add(Convolution3D(filters=64, kernel_size=(3, 3, 3), input_shape=input_shape, padding='same', strides=(1, 1, 1),
                            activation='relu'))
    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))

    # 2nd layer group
    model.add(Convolution3D(filters=128, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))

    # 3rd layer group
    model.add(Convolution3D(filters=256, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(Convolution3D(filters=256, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))

    # 4th layer group
    model.add(Convolution3D(filters=512, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(Convolution3D(filters=512, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))

    # 5th layer group
    model.add(Convolution3D(filters=512, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(Convolution3D(filters=512, kernel_size=(3, 3, 3), padding='same', strides=(1, 1, 1), activation='relu'))
    model.add(ZeroPadding3D(padding=(0, 1, 1)))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))
    model.add(Flatten())

    # FC layers group
    model.add(Dense(units=4096, activation='relu'))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=4096, activation='relu'))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=len(actions), activation='softmax'))

    return model

"""### C-RNN"""

# Convolutional - Recurrent Neural Network


def crnn_model():
    """Build a CNN into RNN.
    Starting version from:
        https://github.com/udacity/self-driving-car/blob/master/
            steering-models/community-models/chauffeur/models.py
    Heavily influenced by VGG-16:
        https://arxiv.org/abs/1409.1556
    Also known as an LRCN:
        https://arxiv.org/pdf/1411.4389.pdf
    """

    def add_default_block(model, kernel_filters, init, reg_lambda):
        # conv
        model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding='same',
                                         kernel_initializer=init, kernel_regularizer=regularizers.l2(reg_lambda))))
        model.add(TimeDistributed(BatchNormalization()))
        model.add(TimeDistributed(Activation('relu')))
        # conv
        model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding='same',
                                         kernel_initializer=init, kernel_regularizer=regularizers.l2(reg_lambda))))
        model.add(TimeDistributed(BatchNormalization()))
        model.add(TimeDistributed(Activation('relu')))
        # max pool
        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))

        return model

    initialiser = 'glorot_uniform'
    reg_lambda = 0.001

    input_shape = (frame_sequences, original_height, original_width, channel)


    model = Sequential()

    # first (non-default) block
    model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), padding='same',
                                     kernel_initializer=initialiser, kernel_regularizer=regularizers.l2(reg_lambda)),
                              input_shape=input_shape))
    model.add(TimeDistributed(BatchNormalization()))
    model.add(TimeDistributed(Activation('relu')))
    model.add(
        TimeDistributed(Conv2D(32, (3, 3), kernel_initializer=initialiser, kernel_regularizer=regularizers.l2(reg_lambda))))
    model.add(TimeDistributed(BatchNormalization()))
    model.add(TimeDistributed(Activation('relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))

    # 2nd-5th (default) blocks
    model = add_default_block(model, 64, init=initialiser, reg_lambda=reg_lambda)
    model = add_default_block(model, 128, init=initialiser, reg_lambda=reg_lambda)
    model = add_default_block(model, 256, init=initialiser, reg_lambda=reg_lambda)
    model = add_default_block(model, 512, init=initialiser, reg_lambda=reg_lambda)

    # LSTM output head
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(256, return_sequences=False, dropout=0.5))
    model.add(Dense(len(actions), activation='softmax'))

    return model


"""### Pose-RNN"""

# Pose Features - Recurrent Neural Network

def posernn_model():
    input_shape = (frame_sequences, channel)
    model = Sequential()
    model.add(LSTM(2048, return_sequences=False,
                   input_shape=input_shape,
                   dropout=0.5))
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(actions), activation='softmax'))

    return model


# fungsi induk dari create_plot, menentukan matrik apa saja yang akan di plot
def visualize(hist, nb_epoch):
    def create_plot(hist, xc, title):
        a = hist.history[title]
        b = hist.history['val_' + title]
        plt.figure()
        plt.plot(xc, a)
        plt.plot(xc, b)
        plt.xlabel('epoch')
        plt.ylabel(title)
        plt.title('train_' + title + ' vs test_' + title)
        plt.grid(True)
        plt.legend(['train', 'test'])
        plt.savefig('graph-{}-{}.png'.format(scenario, title))

    xc = range(nb_epoch)
    create_plot(hist, xc, 'loss')
    create_plot(hist, xc, 'acc')


"""## Model training & validation"""

train_x, test_x, train_y, test_y = train_test_split(dataset, labels, test_size=test_size, random_state=random_state)

if scenario == 1:
    model = c3d_model()
elif scenario == 2:
    model = crnn_model()
elif scenario == 3:
    model = posernn_model()


model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

with open('summary-{}.txt'.format(scenario), 'w') as fh:
    model.summary(print_fn=lambda x: fh.write(x + '\n'))
plot_model(model, to_file='plot_model-{}.txt'.format(scenario), show_shapes=True, show_layer_names=True)

start = datetime.datetime.now()
# melakukan proses training
hist = model.fit(train_x, train_y, validation_data=(test_x, test_y), callbacks=[CSVLogger('log-{}.csv'.format(scenario), separator=';')], batch_size=batch_size, epochs=number_epoch, verbose=1)
end = datetime.datetime.now()
interval = end - start
f = open('info-{}.txt'.format(scenario), 'w')
info = '\nScenario : '.format(scenario)
info += '\nProcess time: ' + str(interval)
print(info)
f.write(info)
visualize(hist, number_epoch)
# menyimpan model
model.save('model-{}.hdf5'.format(scenario))

"""## Test on sample video"""
